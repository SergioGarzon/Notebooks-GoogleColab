{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOOW7uMCije50AY955JwYYz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":41,"metadata":{"id":"FmtSwEJhKrXx","executionInfo":{"status":"ok","timestamp":1727704868520,"user_tz":180,"elapsed":3554,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}}},"outputs":[],"source":["# Instalamos el JDK 8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null"]},{"cell_type":"code","source":["# Descargamos Spark\n","!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz"],"metadata":{"id":"hdmASX84Kvsl","executionInfo":{"status":"ok","timestamp":1727704794664,"user_tz":180,"elapsed":1652,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# Desacomprimimos archivos de Spark\n","!tar xf spark-3.1.1-bin-hadoop3.2.tgz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aW1DGgIWKyeV","executionInfo":{"status":"ok","timestamp":1727704803513,"user_tz":180,"elapsed":377,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}},"outputId":"d2761055-3e73-4ce5-990d-80ad305c3819"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["tar: spark-3.1.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n"]}]},{"cell_type":"code","source":["# Se instala findSpark (usado para ubicar Spark y dejarlo disponible al interprete de Python)\n","!pip install -q findspark"],"metadata":{"id":"1GSH4aP5K0ea","executionInfo":{"status":"ok","timestamp":1727704809223,"user_tz":180,"elapsed":3568,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# Se crean variables de entorno\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""],"metadata":{"id":"k0G0oxpOK2Vu","executionInfo":{"status":"ok","timestamp":1727704811950,"user_tz":180,"elapsed":5,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# Se instala PySpark\n","!pip install -q pyspark"],"metadata":{"id":"-uKskKtVK370","executionInfo":{"status":"ok","timestamp":1727704824255,"user_tz":180,"elapsed":9671,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# Se verifica la instalación\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","Spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453},"id":"I_2wPwC4K6A7","executionInfo":{"status":"error","timestamp":1727704827223,"user_tz":180,"elapsed":358,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}},"outputId":"fe61116f-fc55-4d21-d9c9-ec2460ea8c0d"},"execution_count":40,"outputs":[{"output_type":"error","ename":"Exception","evalue":"Unable to find py4j in /content/spark-3.1.1-bin-hadoop3.2/python, your SPARK_HOME may not be configured correctly","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-d1d17df9ded6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Se verifica la instalación\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mSpark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             raise Exception(\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[1;32m    163\u001b[0m                     \u001b[0mspark_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-3.1.1-bin-hadoop3.2/python, your SPARK_HOME may not be configured correctly"]}]},{"cell_type":"code","source":["# Se prueba la session de Spark\n","df = Spark.createDataFrame([{\"Hola\": \"Mundo\"} for x in range(10)])\n","df.show(10, False)"],"metadata":{"id":"JZrkA5fJK_En"},"execution_count":null,"outputs":[]}]}