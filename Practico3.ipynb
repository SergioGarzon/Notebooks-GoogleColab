{"cells":[{"cell_type":"markdown","metadata":{"id":"g_5lCR5tGmRT"},"source":["**texto en negrita**# Práctico 3: Aprendizaje Supervisado\n","\n","## Mentoría *Sesgos Cognitivos en Razonamientos Lógicos*\n","\n","**Fecha de entrega:** 11/09\n","\n","\n","Aplicaremos algoritmos de regresión de aprendizaje supervisado para predecir los índices de creencias en razonamientos lógicos y así detectar posibles sesgos.\n","\n","- Comenzaremos utilizando un modelo base (baseline model) como referencia para comparar con otros modelos en la resolución de nuestro problema.\n","\n","- Realizaremos optimización de hiperpárametros utilizando técnicas como grid search y random search.\n","\n","- Seleccionaremos métricas de error como MAE, MSE y RMSE, para evaluar nuestros modelos\n","\n","- Opcionalmente, veremos cómo transformar nuestro problema de regresión en un problema de clasificación binaria\n","\n","------------------------------------------------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"Cll_Ia6fGmRV"},"source":["## 1. Selección de características y división en conjunto de entrenamiento y conjunto de prueba\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"ESTTyNf_BelO"},"outputs":[],"source":["# importación de librerías\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.dummy import DummyRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import RandomizedSearchCV"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"jgRr-Um3GmRV","colab":{"base_uri":"https://localhost:8080/","height":213},"executionInfo":{"status":"error","timestamp":1694382032219,"user_tz":180,"elapsed":1018,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}},"outputId":"c3bd318e-0bee-48ec-a897-ba0824b9d522"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-abe2e4f4fd08>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datos_preprocesados_grupo_1.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datos_preprocesados.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["df1 = pd.read_csv(\"datos_preprocesados_grupo_1.csv\")\n","df2 = pd.read_csv(\"datos_preprocesados.csv\")\n","df1"]},{"cell_type":"markdown","metadata":{"id":"wQssXQQAGmRW"},"source":["### 1.1. Indicar qué características se utilizaran durante el entrenamiento"]},{"cell_type":"markdown","metadata":{"id":"J0it49OcGmRW"},"source":["En el trabajo práctico anterior realizamos la curación de datos que creimos pertinente, obteniendo los datos arriba mostrados. A las columnas al estilo %_ValidezxCreencia se les aplicó una codificación ordinal, lo cuál puede ser que no haya sido la mejor decisión dado que no existe una relación jerárquica entre los distintos valores que tomaba esta variable. Al contar ahora con un nuevo dataframe preprocesado, se nos ocurre realizar el entrenamiento con ambos datasets, para evaluar la influencia de esta decisión en el entrenamiento de los modelos, y optar finalmente por el camino que de mejores resultados.<br>\n","Para el entrenamiento descartaremos las columnas relacionadas a los sujetos, como la aceptación y la corrección de los silogismos, dado que al estar estrechamente relacionado con la variable target, estaríamos overfitteando e introduciendo sesgos en nuestro propio análisis de sesgos.\n","Haremos una pequeña curación de datos al nuevo dataset obtenido y luego podremos alternar entre uno y otro."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"-TTw8GE1BelW"},"outputs":[],"source":["#Eliminar Aceptación y correctas\n","aceptacion = ['1_Aceptación', '2_Aceptación', '3_Aceptación', '4_Aceptación',\n","       '5_Aceptación', '6_Aceptación', '7_Aceptación', '8_Aceptación']\n","correctas = ['1_Correctas', '2_Correctas', '3_Correctas', '4_Correctas',\n","       '5_Correctas', '6_Correctas', '7_Correctas', '8_Correctas']\n","\n","df1.drop(aceptacion+correctas, axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"_kc3NlAnBelX","outputId":"cbc69d07-d9ec-479d-fd6e-aff3755b0104"},"outputs":[{"data":{"text/plain":["Index(['Participante', 'Modalidad', '1_ValidezxCreencia', '2_ValidezxCreencia',\n","       '3_ValidezxCreencia', '4_ValidezxCreencia', '5_ValidezxCreencia',\n","       '6_ValidezxCreencia', '7_ValidezxCreencia', '8_ValidezxCreencia',\n","       'indice_creencia_norm'],\n","      dtype='object')"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["df1.columns"]},{"cell_type":"markdown","metadata":{"id":"dmGifoHwGmRX"},"source":["### 1.2. Utilizar esta sección para llevar a cabo cualquier acción que consideren necesaria para pasar a la división y el posterior entrenamiento\n","\n","**Nota**: Recuerden insertar comentarios en el código indicando las acciones que llevan a cabo con su justificación"]},{"cell_type":"markdown","metadata":{"id":"jP9fEMeTBela"},"source":["En este apartado trabajamos sobre el nuevo datadrame adquirido, realizando una pequeña curación para el trabajo del mismo. <br>\n","Primero transformamos los valores categóricos que toman las variables tipo \"Validez_sil%\" y \"Creencia_sil%\" por valores numéricos. La convención utilizada es que para un silogismo \"Válido\" o \"Creíble\", la columna toma un valor 1, en caso del silogismo ser \"Inválido\" o \"Increíble\", la columna toma valor 0. De esta forma hacemos un encoding que no jerarquiza ni prioriza un valor sobre otro, como puede llegar a entenderse en el manejo del primer data frame. <br>\n","Luego de esto, eliminamos las columnas \"ValidezxCreencia_sil%\", dado que además de contener valores categóricas, dicha información ya está contenida en otras columnas que son númericas, así que no perdemos información. Por último, eliminamos las variables edad, género y grupo por los mismos motivos que fueron eliminadas en el dataframe original."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v4lUP3M4GmRX"},"outputs":[],"source":["# Definición de la función que transforma los valores obtenidos del df a nuestra convención\n","def transform_codification(value):\n","    if value == 'V' or value == 'C':\n","        return 1\n","    elif value == 'I':\n","        return 0\n","    else:\n","        return value\n","\n","# Aplica la transformación a las columnas de validez y creencia\n","validez_columns = ['Validez_sil1', 'Validez_sil2', 'Validez_sil3', 'Validez_sil4', 'Validez_sil5', 'Validez_sil6', 'Validez_sil7', 'Validez_sil8']\n","creencia_columns = ['Creencia_sil1', 'Creencia_sil2', 'Creencia_sil3', 'Creencia_sil4', 'Creencia_sil5', 'Creencia_sil6', 'Creencia_sil7', 'Creencia_sil8']\n","\n","for col in validez_columns + creencia_columns:\n","    df2[col] = df2[col].apply(transform_codification)\n","\n","# Elimina las columnas de validez x creencia\n","validezxcreencia_columns = ['ValidezxCreencia_sil1', 'ValidezxCreencia_sil2', 'ValidezxCreencia_sil3', 'ValidezxCreencia_sil4', 'ValidezxCreencia_sil5', 'ValidezxCreencia_sil6', 'ValidezxCreencia_sil7', 'ValidezxCreencia_sil8']\n","df2.drop(validezxcreencia_columns, axis=1, inplace=True)\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"FVzbre5QBelc","executionInfo":{"status":"error","timestamp":1694382589256,"user_tz":180,"elapsed":352,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}},"outputId":"ac8973fa-5b46-45a1-ed62-fc27970e3a02"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-2f66b0aae6b6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"]}],"source":["df2.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"Tu-BZtO9Beld"},"outputs":[],"source":["# Elimina las columnas extras que eliminamos en el análisis del práctico 2\n","drop_extras = [ 'Edad', 'Género', 'Grupo']\n","df2.drop(drop_extras, axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"SwiRNe3CBeld"},"source":["Como dijimos en el punto 1.2, eliminamos las variables relacionadas a 'Aceptación' y 'Correctas'"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"R954Y2lbBele"},"outputs":[],"source":["aceptacion2 = ['Aceptación_sil1', 'Aceptación_sil2', 'Aceptación_sil3', 'Aceptación_sil4',\n","               'Aceptación_sil5', 'Aceptación_sil6', 'Aceptación_sil7', 'Aceptación_sil8']\n","correctas2 = ['Correctas_sil1', 'Correctas_sil2', 'Correctas_sil3', 'Correctas_sil4',\n","              'Correctas_sil5', 'Correctas_sil6', 'Correctas_sil7', 'Correctas_sil8']\n","df2.drop(aceptacion2+correctas2, axis=1, inplace=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"fo9ufy7RBele","executionInfo":{"status":"error","timestamp":1694382655632,"user_tz":180,"elapsed":329,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}},"outputId":"6fed7399-eab6-4d3c-b7ed-b68cf1d1beb5"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2f66b0aae6b6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"]}],"source":["df2.columns"]},{"cell_type":"markdown","metadata":{"id":"sxq1jrV0GmRX"},"source":["## 1.3. Dividir en conjunto de entrenamiento y prueba"]},{"cell_type":"markdown","metadata":{"id":"hmTtaN66Belf"},"source":["Ahora sí realizamos una copia de ambos dataframes en la variable df, así que para entrenar el modelo usamos la misma variable, con la diferencia que comentamos o descomentamos el df que vamos a utilizar. Luego de esto hacemos la selección de los atributos para el entrenamiento y la división del conjunto en entrenamiento y prueba"]},{"cell_type":"code","execution_count":5,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":213},"id":"C-vmAMRYBelf","executionInfo":{"status":"error","timestamp":1694382718846,"user_tz":180,"elapsed":9,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}},"outputId":"c52101d9-bd21-436c-b024-da5bbd63bc4a"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-d79c5c939116>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Hago una copia del datadrame utilizado para el entrenamiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#df = df1.copy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"]}],"source":["# Hago una copia del datadrame utilizado para el entrenamiento\n","#df = df1.copy()\n","df = df2.copy()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"6rBp_0jLGmRY","colab":{"base_uri":"https://localhost:8080/","height":213},"executionInfo":{"status":"error","timestamp":1694382728288,"user_tz":180,"elapsed":300,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}},"outputId":"fc7bdb76-672b-4c4e-b3bf-46feba332b3d"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-6cf467fba582>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Indicamos las características que se utilizarán (X) y la variable objetivo (y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Participante'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Modalidad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'indice_creencia_norm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'indice_creencia_norm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":["# Indicamos las características que se utilizarán (X) y la variable objetivo (y)\n","X = df.drop(['Participante', 'Modalidad', 'indice_creencia_norm'], axis=1)\n","y = df['indice_creencia_norm']"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"acr0y1UaBelg","outputId":"2d82b0c5-3c38-48bc-b187-0785f4da6340"},"outputs":[{"data":{"text/plain":["Index(['Validez_sil1', 'Validez_sil2', 'Validez_sil3', 'Validez_sil4',\n","       'Validez_sil5', 'Validez_sil6', 'Validez_sil7', 'Validez_sil8',\n","       'Creencia_sil1', 'Creencia_sil2', 'Creencia_sil3', 'Creencia_sil4',\n","       'Creencia_sil5', 'Creencia_sil6', 'Creencia_sil7', 'Creencia_sil8'],\n","      dtype='object')"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["X.columns"]},{"cell_type":"code","execution_count":7,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":196},"id":"9vAAoIuMBelh","executionInfo":{"status":"error","timestamp":1694382761190,"user_tz":180,"elapsed":309,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}},"outputId":"af78b58f-1cf3-4f0e-d5c2-f77f36014548"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-375a29eb0262>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dividir en train y test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"]}],"source":["# Dividir en train y test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)"]},{"cell_type":"markdown","metadata":{"id":"GubAUeayGmRY"},"source":["## 2. Modelo base\n","\n","En cada proyecto, es útil crear un modelo de referencia que implemente un algoritmo muy simple. Esto nos permite comparar nuestros resultados posteriores con el modelo base y ver si estamos mejorando.\n","### 2.1.  Crear un modelo que siempre devuelva el índice de creencia promedio.\n","\n","**Ayuda:** scikit-learn cuenta con la clase [DummyRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html) que es muy útil para esta tarea.\n","_______________"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m586r9aYGmRZ","outputId":"49a7efdc-2cb4-47d8-bba8-04fd25bebd1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Modelo Dummy - Accuracy: -0.0022405646004968194\n","Modelo Dummy - MAE: 0.2254344919786096\n","Modelo Dummy - MSE: 0.0809511638880151\n","Modelo Dummy - RMSE: 0.2845191801759859\n"]}],"source":["# Crear un modelo DummyRegressor que prediga siempre el índice de creencia promedio\n","dummy_model = DummyRegressor(strategy='mean')\n","\n","# Entrenar el modelo\n","dummy_model.fit(X_train, y_train)\n","\n","# Predecir en el conjunto de prueba\n","y_pred_dummy = dummy_model.predict(X_test)\n","\n","# Calcular métricas de error\n","mae_dummy = mean_absolute_error(y_test, y_pred_dummy)\n","mse_dummy = mean_squared_error(y_test, y_pred_dummy)\n","rmse_dummy = np.sqrt(mse_dummy)\n","score_dummy = dummy_model.score(X_test, y_test)\n","\n","print(\"Modelo Dummy - Accuracy:\", score_dummy)\n","print(\"Modelo Dummy - MAE:\", mae_dummy)\n","print(\"Modelo Dummy - MSE:\", mse_dummy)\n","print(\"Modelo Dummy - RMSE:\", rmse_dummy)"]},{"cell_type":"markdown","metadata":{"id":"MJS0-FIzGmRZ"},"source":["### 2.2 Evaluación\n","Una vez que hemos entrenado nuestro modelo base y obtenido predicciones para nuestro conjunto de test, es hora de que evaluamos su performance. Para la evaluación usaremos el [error absoluto medio](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) (MAE por sus siglas en inglés).\n","\n","Adicionalmente, van a tener que elegir otra métrica de error y justificar su elección.\n","____________________"]},{"cell_type":"markdown","metadata":{"id":"eY0JGIaDBeli"},"source":["Como métrica adicional utilizaremos el WMAE (error absoluto medio ponderado). El WMAE es similar al MAE (Mean Absolute Error), pero tiene en cuenta el peso de cada observación. El peso de cada observación se puede utilizar para reflejar la importancia relativa de cada observación. En este caso, estamos asignando un peso de 2 a las observaciones que son mayores que 0, y un peso de 0.5 a las observaciones que son menores que 0. Esto significa que el modelo le dará más importancia a las observaciones que son mayores que 0, dado que estas nos indican presencia de sesgos en nuestras observaciones, y menos importancia a las observaciones que son menores que 0 (que como vimos en el práctico anterior, los valores negativos de la variable target no tenía mucho interés de estudio). El resultado será un número que indica el error del modelo en la predicción de las observaciones, teniendo en cuenta los pesos de las observaciones. Un WMAE más bajo indica que el modelo es más preciso."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z7yGdGIUGmRa","outputId":"5146cff6-db17-4931-c584-58bab83997e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Modelo Dummy - WMAE: 0.19531840259039393\n"]}],"source":["mae_mod_base = float(\"{:.2f}\".format(mae_dummy))\n","\n","# Calcular los pesos\n","weights = []\n","for y_i in y_test:\n","    if y_i > 0:\n","        weights.append(2)\n","    else:\n","        weights.append(0.5)\n","\n","# Calcular el error del modelo con MAE\n","wmae = mean_absolute_error(y_test, y_pred_dummy, sample_weight=weights)\n","\n","print(\"Modelo Dummy - WMAE:\", wmae)"]},{"cell_type":"markdown","metadata":{"id":"lNpaEgAcGmRa"},"source":["Antes de pasar a la siguiente sección vamos a llevar a cabo un pequeño test para ver que nuestro modelo base no sobrepase el valor máximo para el MAE que fijamos en $0.24$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GtVID-YYGmRa"},"outputs":[],"source":["# no modificar esta celda\n","\n","def check_mae(mae):\n","    if mae <= 0.24:\n","        print(f\"MAE:{mae}\")\n","    else:\n","        raise ValueError(f'El MAE es de {mae}, necesitan un MAE menor o igual que 0.24. Revisen las características seleccionadas')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdDoR9toGmRa","outputId":"1517941f-ab7a-407b-9ec0-59dad7778d11"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE:0.23\n"]}],"source":["check_mae(mae_mod_base)"]},{"cell_type":"markdown","metadata":{"id":"AMi-t3TiGmRa"},"source":["## 2. Experimentos"]},{"cell_type":"markdown","metadata":{"id":"wBsYTyGQGmRa"},"source":["### 2.1. Probando diferentes estimadores\n","\n","Utilice tres estimadores diferentes para la predicción del índice de creencia, obtenga las predicciones y realice la evaluación\n","\n","**Nota:** Pueden probar la cantidad de modelos que deseen, pero aquí en la notebook deben registrar sólo tres.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4WXK2RN8GmRa","outputId":"26c25bab-400d-495e-ea75-b1c019337d82"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE: 0.2254344919786096\n","WMAE: 0.19531840259039393\n"]}],"source":["# Primer estimador\n","estimador_1 = GradientBoostingRegressor(n_estimators=5, max_depth=3, random_state=42)\n","\n","# train\n","estimador_1.fit(X_train, y_train)\n","\n","# predicciones\n","y_pred_1 = estimador_1.predict(X_test)\n","\n","# evaluación\n","mae_1 = mean_absolute_error(y_test, y_pred_1)\n","weights_1 = []\n","for y_i in y_test:\n","    if y_i > 0:\n","        weights_1.append(2)\n","    else:\n","        weights_1.append(0.5)\n","wmae_1 = mean_absolute_error(y_test, y_pred_1, sample_weight=weights_1)\n","print(\"MAE:\", mae_1)\n","print(\"WMAE:\", wmae_1)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"HRdsO-T4Belk","outputId":"273c177e-a504-489b-ffd9-76ad9f6928c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE: 0.22490808823529412\n","WMAE: 0.19528197517539125\n"]}],"source":["# Segundo estimador\n","estimator_2 = RandomForestRegressor(random_state=42)\n","\n","# train\n","estimator_2.fit(X_train, y_train)\n","\n","# predicciones\n","y_pred_2 = estimator_2.predict(X_test)\n","\n","# evaluación\n","mae_2 = mean_absolute_error(y_test, y_pred_2)\n","weights_2 = []\n","for y_i in y_test:\n","    if y_i > 0:\n","        weights_2.append(2)\n","    else:\n","        weights_2.append(0.5)\n","wmae_2 = mean_absolute_error(y_test, y_pred_2, sample_weight=weights_2)\n","print(\"MAE:\", mae_2)\n","print(\"WMAE:\", wmae_2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFfLl4RuGmRb","outputId":"a03dab76-7588-4cb8-e8c6-0c02b997efbd"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE 3: 0.27982954545454547\n","WMAE 3: 0.19908256880733946\n"]}],"source":["# Tercer estimador: KNeighborsRegressor\n","estimador_3 = KNeighborsRegressor(n_neighbors=5)\n","\n","# train\n","estimador_3.fit(X_train, y_train)\n","\n","# Predicciones\n","y_pred_3 = estimador_3.predict(X_test)\n","\n","# evaluación\n","mae_3 = mean_absolute_error(y_test, y_pred_3)\n","weights_3 = []\n","for y_i in y_test:\n","    if y_i > 0:\n","        weights_3.append(2)\n","    else:\n","        weights_3.append(0.5)\n","wmae_3 = mean_absolute_error(y_test, y_pred_3, sample_weight=weights_3)\n","print(\"MAE 3:\", mae_3)\n","print(\"WMAE 3:\", wmae_3)"]},{"cell_type":"markdown","metadata":{"id":"jjZxWewEGmRb"},"source":["### 2.2. Optimización de hiperparámetros\n"]},{"cell_type":"markdown","metadata":{"id":"RMGQvTd6GmRb"},"source":["Seleccione un estimador de los utilizados en el punto anterior y lleve a cabo una optimización de hiperparámetros utilizando  [Grid Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) o [Random Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html).\n","\n","Registre las mejores métricas alcanzadas y los valores de los hiperparámetros utilizados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4EJd0GlcGmRb","outputId":"c827f04d-43d7-416e-a2cd-fd4512cea0d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mejores hiperparámetros: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n","Mejor MAE: 0.23547458842214422\n","MAE del modelo optimizado: 0.2244986631016043\n","WMAE del modelo optimizado: 0.19525364274150028\n"]}],"source":["estimator_2 = RandomForestRegressor(random_state=42)\n","\n","# Hiperparámetros a ajustar\n","param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [None, 5, 10, 15, 20, 25, 30],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","\n","# Crear un objeto GridSearchCV\n","grid_search = GridSearchCV(estimator_2, param_grid, cv=5, scoring='neg_mean_absolute_error')\n","grid_search.fit(X_train, y_train)\n","\n","# Obtener los mejores hiperparámetros y la mejor puntuación\n","best_params = grid_search.best_params_\n","best_mae = -grid_search.best_score_\n","\n","# Modelo con los mejores hiperparámetros\n","estimator_opt = RandomForestRegressor(**best_params, random_state=42)\n","estimator_opt.fit(X_train, y_train)\n","\n","# predicciones\n","y_pred = estimator_opt.predict(X_test)\n","\n","# evaluacion\n","mae_est_opt = mean_absolute_error(y_test, y_pred)\n","wmae_est_opt = mean_absolute_error(y_test, y_pred, sample_weight=weights)\n","\n","# Imprimir los resultados\n","print(\"Mejores hiperparámetros:\", best_params)\n","print(\"Mejor MAE:\", best_mae)\n","print(\"MAE del modelo optimizado:\", mae_est_opt)\n","print(\"WMAE del modelo optimizado:\", wmae_est_opt)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"gZ9SvXB-Belm","outputId":"f7cc39e0-3845-4399-cadd-36e192217f9a"},"outputs":[{"ename":"ValueError","evalue":"\nAll the 300 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_regression.py\", line 217, in fit\n    return self._fit(X, y)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 454, in _fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\pandas\\core\\generic.py\", line 2064, in __array__\n    return np.asarray(self._values, dtype=dtype)\nValueError: could not convert string to float: 'Resolución Individual Pre'\n\n--------------------------------------------------------------------------------\n270 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_regression.py\", line 217, in fit\n    return self._fit(X, y)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 454, in _fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\pandas\\core\\generic.py\", line 2064, in __array__\n    return np.asarray(self._values, dtype=dtype)\nValueError: could not convert string to float: 'Resolución Individual Post'\n","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[113], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(estimador, param_distributions\u001b[38;5;241m=\u001b[39mparam_dist, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Realizar la búsqueda de hiperparámetros en los datos de entrenamiento\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Obtener los mejores hiperparámetros y la mejor puntuación\u001b[39;00m\n\u001b[0;32m     15\u001b[0m best_params \u001b[38;5;241m=\u001b[39m random_search\u001b[38;5;241m.\u001b[39mbest_params_\n","File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1767\u001b[0m     \u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1768\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1770\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    849\u001b[0m     )\n\u001b[1;32m--> 851\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n","File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n","\u001b[1;31mValueError\u001b[0m: \nAll the 300 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_regression.py\", line 217, in fit\n    return self._fit(X, y)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 454, in _fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\pandas\\core\\generic.py\", line 2064, in __array__\n    return np.asarray(self._values, dtype=dtype)\nValueError: could not convert string to float: 'Resolución Individual Pre'\n\n--------------------------------------------------------------------------------\n270 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_regression.py\", line 217, in fit\n    return self._fit(X, y)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 454, in _fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n  File \"C:\\Users\\lisai\\miniconda3\\lib\\site-packages\\pandas\\core\\generic.py\", line 2064, in __array__\n    return np.asarray(self._values, dtype=dtype)\nValueError: could not convert string to float: 'Resolución Individual Post'\n"]}],"source":["estimador = KNeighborsRegressor()\n","param_dist = {\n","    'n_neighbors': np.arange(1, 30),\n","    'weights': ['uniform', 'distance'],\n","    'p': [1, 2]\n","}\n","\n","# Crear un objeto RandomizedSearchCV\n","random_search = RandomizedSearchCV(estimador, param_distributions=param_dist, n_iter=30, cv=10, scoring='neg_mean_absolute_error')\n","\n","# Realizar la búsqueda de hiperparámetros en los datos de entrenamiento\n","random_search.fit(X_train, y_train)\n","\n","# Obtener los mejores hiperparámetros y la mejor puntuación\n","best_params = random_search.best_params_\n","best_mae = -random_search.best_score_\n","\n","# Crear un modelo con los mejores hiperparámetros encontrados\n","estimador_3_opt = KNeighborsRegressor(**best_params)\n","\n","# Entrenamiento con los mejores hiperparámetros\n","estimador_3_opt.fit(X_train, y_train)\n","\n","# Predicciones con el modelo optimizado\n","y_pred_3_opt = estimador_3_opt.predict(X_test)\n","\n","# Evaluación con MAE\n","mae_3_opt = mean_absolute_error(y_test, y_pred_3_opt)\n","weights_3_opt = []\n","for y_i in y_test:\n","    if y_i > 0:\n","        weights_3_opt.append(2)\n","    else:\n","        weights_3_opt.append(0.5)\n","wmae_3_opt = mean_absolute_error(y_test, y_pred_3_opt, sample_weight=weights_3_opt)\n","\n","#Imprimo resultados\n","print(\"Mejores hiperparámetros:\", best_params)\n","print(\"Mejor MAE:\", best_mae)\n","print(\"MAE del modelo optimizado:\", mae_3_opt)\n","print(\"WMAE del modelo optimizado:\", wmae_3_opt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ItnHP2oKGmRc","outputId":"e5dc083e-61da-49c4-d2c8-4560caf5ba0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE:0.22017045454545456\n"]}],"source":["check_mae(mae_3_opt)"]},{"cell_type":"markdown","metadata":{"id":"w6VQ-NlhBeln"},"source":["##### Conclusiones\n","El mejor modelo que pudimos entrenar es el KNeighborsRegressor, obteniendo un MAE de 0.22017 y un WMAE de 0.19495, frente a los scores obtenidos con el modelo dummy (MAE: 0.22543 y WMAE: 0.19532)."]},{"cell_type":"markdown","metadata":{"id":"dj62R8BOGmRc"},"source":["## 3. Ingeniería de características y re-entrenamiento del modelo\n","\n","Como ya habrán podido observar a lo largo de la diplomatura, mucho de los procesos en ciencia de datos son iterativos. Cuando entrenamos modelos, esto implica agregar y eliminar características, modificar el escalado y la codificación, y otros tipos de acciones que nos permitan mejorar la performance de nuestro modelo.\n","\n","Aquí están algunas acciones que pueden llevar a cabo para mejorar el rendimiento del modelo:\n","<br></br>\n","- Agregar nuevas características a los datos. Esto puede ayudar al modelo a aprender más sobre los datos y a hacer mejores predicciones.\n","\n","- Eliminar características irrelevantes de los datos. Esto puede ayudar al modelo a evitar el sobreajuste y a mejorar su generalización.\n","\n","- Modificar el escalado de los datos. Esto puede ayudar al modelo a aprender más rápido y a hacer mejores predicciones.\n","\n","- Modificar la codificación de los datos. Esto puede ayudar al modelo a entender mejor los datos y a hacer mejores predicciones.\n","\n","\n","<br></br>\n","Una vez que hayan realizado alguna/s de estas acciones, deben reentrenar el modelo utilizado en el punto anterior.\n","\n","Finalmente, recuerden registrar las métricas de error, esto les ayudará a determinar si con estas acciones han mejorado el rendimiento del modelo.\n","__________________________"]},{"cell_type":"markdown","metadata":{"id":"eiKdyRJmBelo"},"source":["Cargamos nuevamente el dataset n°2, que fue el que nos dio mejores resultados. En esta instancia, nos quedaremos con las mismas features elegidas pero agregaremos también la columna de 'Modalidad'. El problema es que esta columna contiene datos categóricos sobre la modalidad de resolución de la actividad, por lo que debemos hacer una codificación para que el modelo pueda entrenar con esta columna extra."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"V5d8JzXxBelo","outputId":"82569fe7-6041-4b83-b861-35616986929e"},"outputs":[{"data":{"text/plain":["Index(['Modalidad', 'Validez_sil1', 'Validez_sil2', 'Validez_sil3',\n","       'Validez_sil4', 'Validez_sil5', 'Validez_sil6', 'Validez_sil7',\n","       'Validez_sil8', 'Creencia_sil1', 'Creencia_sil2', 'Creencia_sil3',\n","       'Creencia_sil4', 'Creencia_sil5', 'Creencia_sil6', 'Creencia_sil7',\n","       'Creencia_sil8', 'indice_creencia_norm'],\n","      dtype='object')"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["#Cargamos los datos y eliminamos atributos previamente descartados, a excepción de modalidad\n","df = pd.read_csv(\"datos_preprocesados.csv\")\n","\n","df.drop(validezxcreencia_columns, axis=1, inplace=True)\n","df.drop(drop_extras, axis=1, inplace=True)\n","df.drop(aceptacion2+correctas2, axis=1, inplace=True)\n","df.drop('Participante', axis=1, inplace=True)\n","df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"Nc8H_kPZBelo"},"outputs":[],"source":["#Hacemos la división en train y test, para luego hacer el OHE\n","X = df.drop(['indice_creencia_norm'], axis=1)\n","y = df['indice_creencia_norm']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)"]},{"cell_type":"code","execution_count":8,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":213},"id":"WQXFZiBEBelp","executionInfo":{"status":"error","timestamp":1694388366552,"user_tz":180,"elapsed":1010,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}},"outputId":"6a04fa12-4c54-4ff9-f719-358c04c502f9"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-1981854e678f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Verificamos la cantidad de filas y columnas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m print(f\"\"\"Conjunto de train: {X_train.shape[0]} filas y {X_train.shape[1]} columnas.\n\u001b[0m\u001b[1;32m      3\u001b[0m Conjunto de test: {X_test.shape[0]} filas y {X_test.shape[1]} columnas. \"\"\")\n","\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"]}],"source":["#Verificamos la cantidad de filas y columnas\n","print(f\"\"\"Conjunto de train: {X_train.shape[0]} filas y {X_train.shape[1]} columnas.\n","Conjunto de test: {X_test.shape[0]} filas y {X_test.shape[1]} columnas. \"\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"gQz61izlBelp","outputId":"a2ff15e2-6667-4dfa-a89f-12e593e24b66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Resolución Individual\n","Resolución Grupal\n","Grupal entre sujetos\n","Resolución Individual Post\n","Resolución Individual Pre\n"]}],"source":["print(*X['Modalidad'].unique(), sep='\\n')"]},{"cell_type":"code","execution_count":9,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"Q78qpwuzBelp","executionInfo":{"status":"error","timestamp":1694388370397,"user_tz":180,"elapsed":796,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}},"outputId":"1a21eb49-b721-4cbe-b54a-100d49a4864f"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-ea3bf070a8b2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Realizamos el encoder para cada set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mohe_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmod_enc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mohe_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Modalidad'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmod_enc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mohe_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Modalidad'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'OneHotEncoder' is not defined"]}],"source":["#Realizamos el encoder para cada set\n","ohe_enc = OneHotEncoder(sparse_output=False)\n","mod_enc_train = ohe_enc.fit_transform(X_train[['Modalidad']])\n","mod_enc_test = ohe_enc.transform(X_test[['Modalidad']])"]},{"cell_type":"code","execution_count":10,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":196},"id":"-zQDzojVBelp","executionInfo":{"status":"error","timestamp":1694388377927,"user_tz":180,"elapsed":310,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}},"outputId":"083aa0c4-d49e-4d39-d31a-88987251a063"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-ec291c74f711>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_enc_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_enc_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'mod_enc_train' is not defined"]}],"source":["print(mod_enc_train.shape)\n","print(mod_enc_test.shape)"]},{"cell_type":"code","execution_count":11,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":249},"id":"5HASqOhIBelq","executionInfo":{"status":"error","timestamp":1694388388192,"user_tz":180,"elapsed":674,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}},"outputId":"dac8de2d-acd0-4d53-d848-134232e96593"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-aecca4021b6e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Reseteamos el índice y concatenamos con el df anterior, sin la columna de modalidad ya que dicha columna se encuentra codeada ya\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m X_train_encoded = pd.concat([X_train.drop('Modalidad', axis=1).reset_index(drop=True), \n\u001b[0m\u001b[1;32m      3\u001b[0m                             pd.DataFrame(mod_enc_train)], axis=1)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m X_test_encoded = pd.concat([X_test.drop('Modalidad', axis=1).reset_index(drop=True), \n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["#Reseteamos el índice y concatenamos con el df anterior, sin la columna de modalidad ya que dicha columna se encuentra codeada ya\n","X_train_encoded = pd.concat([X_train.drop('Modalidad', axis=1).reset_index(drop=True),\n","                            pd.DataFrame(mod_enc_train)], axis=1)\n","\n","X_test_encoded = pd.concat([X_test.drop('Modalidad', axis=1).reset_index(drop=True),\n","                           pd.DataFrame(mod_enc_test)], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"ajh7RI4PBelq","outputId":"92d61ac1-198e-4289-bf94-10f8eca9fa01"},"outputs":[{"name":"stdout","output_type":"stream","text":["Conjunto de train con columna categórica codificada: 408 filas y 21 columnas.\n","Conjunto de test con columna categórica codificada: 176 filas y 21 columnas. \n"]}],"source":["#Verificamos que tengamos la cantidad de filas y columnas correctas\n","print(f\"\"\"Conjunto de train con columna categórica codificada: {X_train_encoded.shape[0]} filas y {X_train_encoded.shape[1]} columnas.\n","Conjunto de test con columna categórica codificada: {X_test_encoded.shape[0]} filas y {X_test_encoded.shape[1]} columnas. \"\"\")"]},{"cell_type":"markdown","metadata":{"id":"6lzrGBh9Belr"},"source":["Eran 5 categorías y eliminamos la variable original ('Modalidad') por lo cual deberían haberse agregado 4 columnas (5-1). Como vemos efectivamente es así, además el número de filas se mantuvo intacto, como debe ser."]},{"cell_type":"code","execution_count":12,"metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":249},"id":"6NpKWktABelr","executionInfo":{"status":"error","timestamp":1694388421665,"user_tz":180,"elapsed":317,"user":{"displayName":"Sergio Gabriel Garzon","userId":"10502280143572352479"}},"outputId":"aa791ca5-449d-4100-ee3e-19726d4a997a"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-0e5ab99d2bb1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Le ponemos las etiquetas correctas a las variables codeadas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m X_train_enc_2 = pd.concat([X_train.reset_index(drop=True),\n\u001b[0m\u001b[1;32m      3\u001b[0m                               pd.DataFrame(mod_enc_train, columns = ohe_enc.categories_[0])],  axis = 1)\n\u001b[1;32m      4\u001b[0m X_test_enc_2 = pd.concat([X_test.reset_index(drop=True),\n\u001b[1;32m      5\u001b[0m                               pd.DataFrame(mod_enc_test, columns = ohe_enc.categories_[0])],  axis = 1)\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["#Le ponemos las etiquetas correctas a las variables codeadas\n","X_train_enc_2 = pd.concat([X_train.reset_index(drop=True),\n","                              pd.DataFrame(mod_enc_train, columns = ohe_enc.categories_[0])],  axis = 1)\n","X_test_enc_2 = pd.concat([X_test.reset_index(drop=True),\n","                              pd.DataFrame(mod_enc_test, columns = ohe_enc.categories_[0])],  axis = 1)\n","\n","X_train_enc_2.drop('Modalidad', axis=1, inplace=True)\n","X_test_enc_2.drop('Modalidad', axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"2Dhtv4kdBelr"},"outputs":[],"source":["#Realizamos la codificación de las columnas validez y creencia\n","for col in validez_columns + creencia_columns:\n","    X_train_enc_2[col] = X_train_enc_2[col].apply(transform_codification)\n","\n","for col in validez_columns + creencia_columns:\n","    X_test_enc_2[col] = X_test_enc_2[col].apply(transform_codification)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"zP4DMLr6Bels","outputId":"1c3df999-11ba-4d9f-9618-3aaff8a00dbf"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Validez_sil1</th>\n","      <th>Validez_sil2</th>\n","      <th>Validez_sil3</th>\n","      <th>Validez_sil4</th>\n","      <th>Validez_sil5</th>\n","      <th>Validez_sil6</th>\n","      <th>Validez_sil7</th>\n","      <th>Validez_sil8</th>\n","      <th>Creencia_sil1</th>\n","      <th>Creencia_sil2</th>\n","      <th>...</th>\n","      <th>Creencia_sil4</th>\n","      <th>Creencia_sil5</th>\n","      <th>Creencia_sil6</th>\n","      <th>Creencia_sil7</th>\n","      <th>Creencia_sil8</th>\n","      <th>Grupal entre sujetos</th>\n","      <th>Resolución Grupal</th>\n","      <th>Resolución Individual</th>\n","      <th>Resolución Individual Post</th>\n","      <th>Resolución Individual Pre</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>403</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>404</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>405</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>406</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>407</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>408 rows × 21 columns</p>\n","</div>"],"text/plain":["     Validez_sil1  Validez_sil2  Validez_sil3  Validez_sil4  Validez_sil5  \\\n","0               1             1             0             0             1   \n","1               1             1             0             0             1   \n","2               1             1             0             0             1   \n","3               1             1             0             0             1   \n","4               1             1             0             0             1   \n","..            ...           ...           ...           ...           ...   \n","403             1             1             0             0             1   \n","404             1             1             0             0             1   \n","405             1             1             0             0             1   \n","406             1             1             0             0             1   \n","407             1             1             0             0             1   \n","\n","     Validez_sil6  Validez_sil7  Validez_sil8  Creencia_sil1  Creencia_sil2  \\\n","0               1             0             0              1              0   \n","1               1             0             0              1              0   \n","2               1             0             0              1              0   \n","3               1             0             0              1              0   \n","4               1             0             0              1              0   \n","..            ...           ...           ...            ...            ...   \n","403             1             0             0              1              0   \n","404             1             0             0              1              0   \n","405             1             0             0              1              0   \n","406             1             0             0              1              0   \n","407             1             0             0              1              0   \n","\n","     ...  Creencia_sil4  Creencia_sil5  Creencia_sil6  Creencia_sil7  \\\n","0    ...              0              1              0              1   \n","1    ...              0              1              0              1   \n","2    ...              0              1              0              1   \n","3    ...              0              1              0              1   \n","4    ...              0              1              0              1   \n","..   ...            ...            ...            ...            ...   \n","403  ...              0              1              0              1   \n","404  ...              0              1              0              1   \n","405  ...              0              1              0              1   \n","406  ...              0              1              0              1   \n","407  ...              0              1              0              1   \n","\n","     Creencia_sil8  Grupal entre sujetos  Resolución Grupal  \\\n","0                0                   0.0                0.0   \n","1                0                   0.0                1.0   \n","2                0                   0.0                1.0   \n","3                0                   0.0                1.0   \n","4                0                   0.0                1.0   \n","..             ...                   ...                ...   \n","403              0                   0.0                0.0   \n","404              0                   0.0                1.0   \n","405              0                   0.0                0.0   \n","406              0                   0.0                0.0   \n","407              0                   0.0                0.0   \n","\n","     Resolución Individual  Resolución Individual Post  \\\n","0                      0.0                         1.0   \n","1                      0.0                         0.0   \n","2                      0.0                         0.0   \n","3                      0.0                         0.0   \n","4                      0.0                         0.0   \n","..                     ...                         ...   \n","403                    1.0                         0.0   \n","404                    0.0                         0.0   \n","405                    0.0                         0.0   \n","406                    1.0                         0.0   \n","407                    0.0                         1.0   \n","\n","     Resolución Individual Pre  \n","0                          0.0  \n","1                          0.0  \n","2                          0.0  \n","3                          0.0  \n","4                          0.0  \n","..                         ...  \n","403                        0.0  \n","404                        0.0  \n","405                        1.0  \n","406                        0.0  \n","407                        0.0  \n","\n","[408 rows x 21 columns]"]},"execution_count":111,"metadata":{},"output_type":"execute_result"}],"source":["X_train_enc_2"]},{"cell_type":"markdown","metadata":{"id":"_2wBgOk_Bels"},"source":["Ahora sí, probamos el mejor modelo obtenido (KNeighborsRegressor) con una\n","\n","---\n","\n","optimización de hiperparámetros, para ver si el rendimiento mejora con las nuevas columnas incorporadas."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"T_T3Jz8JBels","outputId":"79b56046-7cc5-4433-86e7-779d0d5692a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAE 3: 0.28740530303030304\n","WMAE 3: 0.26620795107033635\n"]}],"source":["estimador_3 = KNeighborsRegressor(n_neighbors=3)\n","\n","# train\n","estimador_3.fit(X_train_enc_2, y_train)\n","\n","# Predicciones\n","y_pred_3 = estimador_3.predict(X_test_enc_2)\n","\n","# evaluación\n","mae_3 = mean_absolute_error(y_test, y_pred_3)\n","weights_3 = []\n","for y_i in y_test:\n","    if y_i > 0:\n","        weights_3.append(2)\n","    else:\n","        weights_3.append(0.5)\n","wmae_3 = mean_absolute_error(y_test, y_pred_3, sample_weight=weights_3)\n","print(\"MAE 3:\", mae_3)\n","print(\"WMAE 3:\", wmae_3)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"-4trhOhQBelt","outputId":"3ea42116-7723-480c-89d7-ba2d30b267f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mejores hiperparámetros: {'weights': 'uniform', 'p': 2, 'n_neighbors': 18}\n","Mejor MAE: 0.23727981029810294\n","MAE del modelo optimizado: 0.2267992424242424\n","WMAE del modelo optimizado: 0.19877675840978593\n"]}],"source":["estimador = KNeighborsRegressor()\n","param_dist = {\n","    'n_neighbors': np.arange(1, 30),\n","    'weights': ['uniform', 'distance'],\n","    'p': [1, 2]\n","}\n","\n","# Crear un objeto RandomizedSearchCV\n","random_search = RandomizedSearchCV(estimador, param_distributions=param_dist, n_iter=30, cv=10, scoring='neg_mean_absolute_error', random_state=42)\n","\n","# Realizar la búsqueda de hiperparámetros en los datos de entrenamiento\n","random_search.fit(X_train_enc_2, y_train)\n","\n","# Obtener los mejores hiperparámetros y la mejor puntuación\n","best_params = random_search.best_params_\n","best_mae = -random_search.best_score_\n","\n","# Crear un modelo con los mejores hiperparámetros encontrados\n","estimador_3_opt = KNeighborsRegressor(**best_params)\n","\n","# Entrenamiento con los mejores hiperparámetros\n","estimador_3_opt.fit(X_train_enc_2, y_train)\n","\n","# Predicciones con el modelo optimizado\n","y_pred_3_opt = estimador_3_opt.predict(X_test_enc_2)\n","\n","# Evaluación\n","mae_3_opt = mean_absolute_error(y_test, y_pred_3_opt)\n","weights_3_opt = []\n","for y_i in y_test:\n","    if y_i > 0:\n","        weights_3_opt.append(2)\n","    else:\n","        weights_3_opt.append(0.5)\n","wmae_3_opt = mean_absolute_error(y_test, y_pred_3_opt, sample_weight=weights_3_opt)\n","\n","#Imprimimos resultados\n","print(\"Mejores hiperparámetros:\", best_params)\n","print(\"Mejor MAE:\", best_mae)\n","print(\"MAE del modelo optimizado:\", mae_3_opt)\n","print(\"WMAE del modelo optimizado:\", wmae_3_opt)"]},{"cell_type":"markdown","metadata":{"id":"Z4l-1MKKGmRc"},"source":["### 3.1. Evaluación\n","\n","Atendiendo a lo realizado hasta ahora, responda las siguientes preguntas:\n","- ¿Observan diferencias en el rendimiento del modelo base con el que empezaron y el de los diferentes modelos entrenados posteriormente?\n","\n","- ¿Hubo diferencias en el rendimiento del modelo con parámetros optimizados antes y después de trabajar con las características?¿Por qué considera que sucede esto?\n","_____________________"]},{"cell_type":"markdown","metadata":{"id":"V5-12DwSBelt"},"source":["Con el mejor modelo logramos alcanzar un MAE de 0.2202, en contraste con el MAE de 0.2254 obtenido utilizando el modelo dummy. Aunque esta mejora puede parecer modesta a primera vista, consideremos que nuestra variable objetivo se encuentra en el rango de -1 a 1, y en su mayoría toma valores en el intervalo de 0 a 1 (valores pequeños), por lo que esta diferencia es apreciable.\n","\n","Destacamos que el rendimiento de los modelos mejoró significativamente con el conjunto de datos preprocesado adquirido en comparación al dataset preprocesado de trabajos anteriores. Por este motivo, observamos que la decisión de utilizar una codificación ordinal para la variable 'ValidezxCreencia' no fue la más acertada.\n","\n","En cuanto a la inclusión de la variable 'Modalidad', no obtuvimos mejoras significativas. Es posible que esto se deba a la codificación utilizada previamente para las columnas 'Validez' y 'Creencia'. Consideramos que una opción más efectiva podría ser aplicar la codificación 'One-Hot Encoding' (OHE) para cada una de estas columnas. Por ejemplo, en lugar de tener 'Validez_sil1' y 'Creencia_sil1' como columnas individuales, podríamos crear columnas binarias separadas, como 'Validez_sil1_V', 'Validez_sil1_I', 'Creencia_sil1_C', 'Creencia_sil1_I', y así sucesivamente para cada silogismo."]},{"cell_type":"markdown","metadata":{"id":"eq2Y-UmqGmRd"},"source":["## 4. (Opcional) Replanteando nuestro problema\n","\n","A lo largo de esta actividad hemos abordado el problema de predecir el índice de creencia. Este índice es muy útil para observar la polarización en las respuestas y analizar aspectos más sútiles de la problemática.\n","\n","Sin embargo, podríamos simplemente querer predecir si tienen lugar o no dichos sesgos, sin importar su grado, con lo cual podríamos replantear nuestro problema como un problema de clasificación binaria. En este tipo de problemas, el objetivo es predecir si una instancia pertenece a una clase o a otra. En nuestro caso, las dos clases son \"presencia de sesgos\" y \"ausencia de sesgos\".\n","\n","\n","Para hacer esto, podemos modificar nuestra columna con la variable objetivo. Los valores positivos se reemplazarán con la etiqueta $1$, que indicarán la presencia de sesgos. Mientras que, los valores iguales o menores que $0$ se reemplazarán con la etiqueta $0$, que indicarán la ausencia de sesgos.\n","\n","Como ejercicio opcional, los invitamos a que entrenen un modelo que emplee algoritmos de clasificación para predecir la ausencia o presencia de sesgos de creencia."]},{"cell_type":"markdown","metadata":{"id":"N-IV9noXGmRd"},"source":["## 5. Informe"]},{"cell_type":"markdown","metadata":{"id":"jdlH3Cg-GmRd"},"source":["*Elaboren* un breve informe de lo realizado durante esta actividad práctica reseñando aspectos salientes, dificultades encontradas, etc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OMeYwAIwBelu"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}